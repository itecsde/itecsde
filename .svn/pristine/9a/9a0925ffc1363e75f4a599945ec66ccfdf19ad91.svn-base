# encoding: utf-8

require 'capybara/rails'
require "capybara"
require "capybara/dsl"
require "nokogiri"

class ScraperReports
  include ActionView::Helpers::SanitizeHelper
  include Capybara::DSL
  
=begin   
  def scrape_rss_report(rss_feed, *reintentos)
    begin   
      report_name = ""
      info_to_wikify = ""    
      report_summary = ""
      report_content = ""    
      puts "RSS FEED " 
      puts rss_feed
      puts "FIN RSS FEED"
      idiomas_disponibles = []

      if reintentos[0]==1
        reintentos = reintentos[1].to_i
      else
        reintentos = 2
      end
  
      feed = Feedjira::Feed.fetch_and_parse(rss_feed,{ :max_redirects => 3 })
      html_page = Nokogiri::HTML(open(rss_feed))

      begin
        rss_feed_name = feed.title
        rss_feed_description = feed.description
        rss_feed_url_image = html_page.css("image url").text.strip
        puts rss_feed_url_image       
        rss_feed_language = html_page.css("language").text.strip.split("-")[0]
        idiomas_disponibles << rss_feed_language 
      rescue Exception => e
        puts e.message
        puts e.backtrace.inspect
      end
             
      source_name = rss_feed_name    
      #Add new source, or select it if exists
      source = Source.create_or_select_source(source_name, "Report", rss_feed)
      begin
        if rss_feed_url_image != nil          
          source.element_image = URI.parse(rss_feed_url_image)
          source.save
        end
      rescue
      end
             
      # Retrieves the reports, extracts the info, and saves them     
      feed.entries.each_with_index do |reports,index|
        begin
          if Report.where(:url => reports.url).size == 0
            scraped_report = Report.new
            begin
              puts index
              summary_content = ""
              report_name = ""
              report_summary = ""
              report_content = ""
              info_to_wikify = ""
              report_name = reports.title.strip
              report_summary = strip_tags reports.summary
              report_content = strip_tags reports.content
              begin
                report_image_url = reports.image
              rescue
              end
              if report_image_url == nil
                begin
                  report_image_url = reports.enclosure_url
                rescue
                end
              end
              if report_image_url == nil
                begin
                  report_image_url = reports.enclosure_url
                rescue
                end
              end
              report_url = reports.url
              report_publication_date = reports.published
              if report_summary != nil
                summary_content = summary_content + report_summary
              end
              if report_content != nil
                summary_content = summary_content + ". " + report_content
              end
            rescue Exception => e
              puts e.message
              puts e.backtrace.inspect
            end
                              
            return_values = extract_info_from_report(report_url, report_name)
            if return_values != nil
              image_url = return_values[:image_url]
              info_to_wikify = return_values[:info_to_wikify]
            end
            if info_to_wikify == nil
              info_to_wikify = summary_content
              description = info_to_wikify
            elsif info_to_wikify.size < summary_content.size
              info_to_wikify = summary_content
              description = info_to_wikify
            end 
            
            if image_url == nil
              image_url = report_image_url
            end

            if image_url != nil          
              scraped_report.element_image = URI.parse(image_url)
            end
            
            I18n.locale = idiomas_disponibles[0]
            scraped_from = rss_feed
            scraped_report.name = report_name
            scraped_report.description = info_to_wikify
            scraped_report.publication_date = report_publication_date
            scraped_report.url = report_url
            scraped_report.link = report_url
            scraped_report.info_to_wikify = ""

            if report_name != nil
              scraped_report.info_to_wikify = scraped_report.info_to_wikify + report_name
            end
            if info_to_wikify != nil
              scraped_report.info_to_wikify = scraped_report.info_to_wikify + info_to_wikify
            end
            
            scraped_report.scraped_from = scraped_from
            scraped_report.persist(idiomas_disponibles,scraped_report.info_to_wikify,[])
          else
            puts "Report ya existente."
          end
        rescue Exception => e
          puts "Failed Report"
          puts e.message
          puts e.backtrace.inspect
        end
      end 
      #After correct finish, store the source in sources
      source.correct_scrape
    rescue  Exception => e
      puts "Failed scrape_rss_report"
      puts e.message
      puts e.backtrace.inspect
      if reintentos > 0
        puts reintentos
        reintentos -= 1
        sleep 2
        scrape_rss_report(rss_feed, 1, reintentos)
      end
      source.incorrect_scrape
    end   
  end #scrape_rss_report
  
  def extract_info_from_report(report_url, report_title)
    begin
      return_values = Hash.new
      puts report_url
      puts report_title
      # Builds report object
      report_html = open(report_url,"User-Agent" => "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:26.0) Gecko/20100101 Firefox/26.0").read
      report_html.gsub!("\n", "")
      report_object = Nokogiri::HTML(report_html)      
      # Cleans it
      report_object.at_css("body").traverse do |node|
        if node.text?
          node.content = node.content.gsub("\r", "")
          node.content = node.content.gsub("\n", "")
          node.content = node.content.gsub("\t", "")
          node.content = node.content.strip
        end
      end
      report_object.encoding = 'UTF-8'
      
      begin     
      # Finds nodes that contain post_title
      report_title_matches = report_object.css("body")[0].search('[text()="' + report_title.strip + '"]')
      rescue Exception => e
        puts "Failed matches report title"
        puts e.message
        puts e.backtrace.inspect
      end
  
      # Gets the most suitable node
      best_match = report_title_matches[0]
      puts best_match
      if report_title_matches[0].parent.name == "li" or report_title_matches[0].parent.parent.name == "li" or report_title_matches[0].parent.parent.parent.name == "li" 
        best_match = report_title_matches[1]
      end
  
      # Extracts a representative image and the content of the post
      if best_match != nil
        if !best_match.parent.css("img").empty?
          # Gets an array of images
          image_matches = best_match.parent.css("img")
          # Gets the first image with size higher than 2k
          image_url = first_big_image(report_url, image_matches)
          info_to_wikify = ""
          best_match.parent.css("p").each do |parrafo|
            if parrafo['class'] == nil
              info_to_wikify = info_to_wikify + parrafo.text
            end
          end
        end
        if image_url == nil
          if !best_match.parent.parent.css("img").empty?
            # Gets an array of images
            image_matches = best_match.parent.parent.css("img")
            # Gets the first image with size higher than 2k
            image_url = first_big_image(report_url, image_matches)
            info_to_wikify = ""
            best_match.parent.parent.css("p").each do |parrafo|
              if parrafo['class'] == nil
                info_to_wikify = info_to_wikify + parrafo.text
              end
            end
          end
          if image_url == nil
            if !best_match.parent.parent.parent.css("img").empty?
              # Gets an array of images
              image_matches = best_match.parent.parent.parent.css("img")
              info_to_wikify = strip_tags best_match.parent.parent.parent.text.gsub(/\n/," ").gsub(/\s+/,' ').gsub("&"," ").gsub("%"," ")
              # Gets the first image with size higher than 2k
              image_url = first_big_image(report_url, image_matches)
              info_to_wikify = ""
              best_match.parent.parent.parent.css("p").each do |parrafo|
                if parrafo['class'] == nil
                  info_to_wikify = info_to_wikify + parrafo.text
                end
              end
            end
          end
        end
        if info_to_wikify == "" or info_to_wikify == nil
          info_to_wikify = ""
          best_match.parent.css("p").each do |parrafo|
            if parrafo['class'] == nil          
              info_to_wikify = info_to_wikify + parrafo.text
            end
          end
        end
        if info_to_wikify == "" or info_to_wikify == nil
          info_to_wikify = ""
          best_match.parent.parent.css("p").each do |parrafo|
            if parrafo['class'] == nil          
              info_to_wikify = info_to_wikify + parrafo.text
            end
          end
        end
        if info_to_wikify == "" or info_to_wikify == nil
          info_to_wikify = ""
          best_match.parent.parent.parent.css("p").each do |parrafo|
            if parrafo['class'] == nil          
              info_to_wikify = info_to_wikify + parrafo.text
            end
          end
        end         
      end
      return_values = {:image_url => image_url, :info_to_wikify => info_to_wikify}
    rescue
      puts "Failed extract_info_from_post -> reports"
    end
  end
     
  def first_big_image(report_url, image_matches)
    begin
      image_url = nil
      image_matches.each do |match|
        dummy_report = Report.new
        # Builds the URL
        if match['src'] != nil
          url = match['src']
          if !match['src'].include? "http"
            url = "http://" + URI.parse(report_url).host + match['src']
          end
          # Parses it
          dummy_report.element_image = URI.parse(url)
          # Assigns the return value if the image is bigger than 2k
          if dummy_report.element_image.size > 2500
            image_url = url
            break
          end
        end
      end
      return image_url
    rescue Exception => e
      puts "Failed first_big_image"
      puts e.message
      puts e.backtrace.inspect
    end
  end
=end  
  
  
end