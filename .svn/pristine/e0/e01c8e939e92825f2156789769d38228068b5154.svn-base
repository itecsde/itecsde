# encoding: utf-8

require 'capybara/rails'
require "capybara"
require "capybara/dsl"

class ScraperReports
  include ActionView::Helpers::SanitizeHelper
  include Capybara::DSL
  
  #############################################################
  ####### SCRAPE_EL_PAIS
  ###########################################################
  def scrape_elpais
    rss_feed = "http://ep00.epimg.net/rss/tags/c_ciencia.xml"
    scrape_rss_report rss_feed
      
    source_name = __method__.to_s
    if Source.find_by_name(source_name) != nil
      source = Source.find_by_name(source_name)
      source.number_of_resources = Report.where(:scraped_from => rss_feed).size
      source.last_scraping = Time.now
      source.save
    else
      source = Source.new
      source.name = source_name
      source.source_type = "Report"
      source.scraped_from = rss_feed
      source.number_of_resources = Report.where(:scraped_from => rss_feed).size
      source.last_scraping = Time.now
      source.url = "http://ep00.epimg.net/rss/tags/c_ciencia.xml"
      source.save
    end 
  end
   
  def scrape_reuters
    rss_feed = "http://feeds.reuters.com/reuters/scienceNews"
    scrape_rss_report rss_feed
      
    source_name = __method__.to_s
    if Source.find_by_name(source_name) != nil
      source = Source.find_by_name(source_name)
      source.number_of_resources =Report.where(:scraped_from => rss_feed).size
      source.last_scraping = Time.now
      source.save
    else
      source = Source.new
      source.name = source_name
      source.source_type = "Report"
      source.number_of_resources = Report.where(:scraped_from => rss_feed).size
      source.last_scraping = Time.now
      source.url = "http://ep00.epimg.net/rss/tags/c_ciencia.xml"
      source.save
    end 
  end 
   
   
  def scrape_rss_report(rss_feed, *reintentos)
    begin   
      report_name = ""
      info_to_wikify = ""    
      report_summary = ""
      report_content = ""    
      puts "RSS FEED " 
      puts rss_feed
      puts "FIN RSS FEED"
      idiomas_disponibles = ["en"]
      
      if reintentos[0]==1
        reintentos = reintentos[1].to_i
      else
        reintentos = 2
      end

      human_tags = Array.new
   
      # Reads the feed
      feed = Feedzirra::Feed.fetch_and_parse(rss_feed,{ :max_redirects => 3 })

      begin
        report_name = feed.title
        report_description = feed.description
      rescue Exception => e
        puts e.message
        puts e.backtrace.inspect
      end
             
      # Retrieves the posts, extracts the info, and saves them
      
      feed.entries.each_with_index do |reports,index|
      begin
        scraped_report = Report.new
        begin
          puts index
          report_name = reports.title.strip
          report_summary = strip_tags reports.summary
          report_content = strip_tags reports.content
          report_url = reports.url
        rescue Exception => e
          puts e.message
          puts e.backtrace.inspect
        end
           
           
        return_values = extract_info_from_report(report_url, report_name)
        image_url = return_values[:image_url]
        info_to_wikify = return_values[:info_to_wikify]
        puts
        puts "INFO TO WIKIFY"
        puts info_to_wikify 
        puts "END INFO TO WIKIFY"
        puts  
        
        if image_url != nil          
          scraped_report.element_image = URI.parse(image_url)
        end
        
        I18n.locale = idiomas_disponibles[0]

        scraped_from = rss_feed
        scraped_report.name = report_name
        #scraped_report.description = report_content
        scraped_report.description = info_to_wikify
        scraped_report.url = report_url
        scraped_report.link = report_url
        #scraped_report.info_to_wikify = report_name + ". " + report_content
        scraped_report.info_to_wikify = "" 
        scraped_report.scraped_from = scraped_from
        scraped_report.persist(idiomas_disponibles,scraped_report.info_to_wikify,[])
      rescue Exception => e
        puts "Failed Report"
        puts e.message
        puts e.backtrace.inspect
      end
    end 
    rescue  Exception => e
      puts e.backtrace.inspect
      puts e.message
      if reintentos > 0
        puts reintentos
        reintentos -= 1
        sleep 2
        scrape_rss_report(rss_feed, 1, reintentos)
      end
    end   
  end #scrape_rss_report
  
  def extract_info_from_report(report_url, report_title)
    return_values = Hash.new
    puts report_url
    puts report_title
    # Builds report object
    report_html = open(report_url).read
    report_html.gsub!("\n", "")
    report_object = Nokogiri::HTML(report_html)
    
    # Cleans it
    report_object.at_css("body").traverse do |node|
      if node.text?
        node.content = node.content.gsub("\r", "")
        node.content = node.content.gsub("\n", "")
        node.content = node.content.gsub("\t", "")
        node.content = node.content.strip
      end
    end
    report_object.encoding = 'UTF-8'
    
    begin     
    # Finds nodes that contain post_title
    report_title_matches = report_object.css("body")[0].search('[text()="' + report_title.strip + '"]')
    rescue Exception => e
      puts "Failed matches report title"
      puts e.message
      puts e.backtrace.inspect
    end

    # Gets the most suitable node
    best_match = report_title_matches[0]
    if report_title_matches[0].parent.name == "li"
      best_match = report_title_matches[1]
    end

    # Extracts a representative image and the content of the post
    if best_match != nil
      if !best_match.parent.css("img").empty?
        # Gets an array of images
        image_matches = best_match.parent.css("img")
        # Gets the first image with size higher than 2k
        image_url = first_big_image(report_url, image_matches)
        #info_to_wikify = strip_tags best_match.parent.text.gsub(/\n/," ").gsub(/\s+/,' ').gsub("&"," ").gsub("%"," ")
        info_to_wikify = ""
        best_match.parent.css("p").each do |parrafo|
          info_to_wikify = info_to_wikify + parrafo.text
        end
      end
      if image_url == nil
        if !best_match.parent.parent.css("img").empty?
          # Gets an array of images
          image_matches = best_match.parent.parent.css("img")
          image_url = first_big_image(report_url, image_matches)
          #info_to_wikify = strip_tags best_match.parent.parent.text.gsub(/\n/," ").gsub(/\s+/,' ').gsub("&"," ").gsub("%"," ")
          info_to_wikify = ""
          best_match.parent.parent.css("p").each do |parrafo|
            info_to_wikify = info_to_wikify + parrafo.text
          end
        end
        if image_url == nil
          if !best_match.parent.parent.parent.css("img").empty?
            # Gets an array of images
            image_matches = best_match.parent.parent.parent.css("img")
            info_to_wikify = strip_tags best_match.parent.parent.parent.text.gsub(/\n/," ").gsub(/\s+/,' ').gsub("&"," ").gsub("%"," ")
            image_url = first_big_image(report_url, image_matches)
            info_to_wikify = ""
            best_match.parent.parent.parent.css("p").each do |parrafo|
              info_to_wikify = info_to_wikify + parrafo.text
            end
          end
        end
      end          
    end
    return_values = {:image_url => image_url, :info_to_wikify => info_to_wikify}
  end
     
  def first_big_image(report_url, image_matches)
    begin
    image_url = nil
    image_matches.each do |match|
      dummy_report = Report.new
      # Builds the URL
      url = match['src']
      if !match['src'].include? "http://"
        url = "http://" + URI.parse(report_url).host + match['src']
      end
      # Parses it
      dummy_report.element_image = URI.parse(url)
      # Assigns the return value if the image is bigger than 2k
      if dummy_report.element_image.size > 2000
        image_url = url
        break
      end
    end
    return image_url
    rescue Exception => e
      puts "Failed first_big_image"
      puts e.message
      puts e.backtrace.inspect
    end
  end
  
  
  
end