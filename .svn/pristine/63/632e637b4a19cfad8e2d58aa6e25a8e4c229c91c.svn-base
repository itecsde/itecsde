# encoding: utf-8

require 'capybara/rails'
require "capybara"
require "capybara/dsl"
require 'feedzirra'

class ScraperBlogs
  include ActionView::Helpers::SanitizeHelper
  include Capybara::DSL
  
  ###############################################################
  #######  SCRAPE_BOTW (BETS OF DE THE WEB BLOGS)
  ###############################################################

  def scrape_botw(*reintentos)
    begin

      if reintentos[0]==1
        reintentos = reintentos[1].to_i
      else
        reintentos = 2
      end
      
      i=0
      url = "http://blogs.botw.org/"   
      page = Nokogiri::HTML(open(url))
          
      page.css("div.categories-holder ul.category-list li.category").each do |category|
        category_page = Nokogiri::HTML(open("http://blogs.botw.org" + category.css("a")[0]['href']))
        puts category.css("a")[0]['href']
        if !category_page.css(".categories-empty li a").empty?
          puts "tiene subcategorias"
          category_page.css(".categories-empty li a").each do |subcategory|
            subcategory_page = Nokogiri::HTML(open("http://blogs.botw.org" + subcategory['href']))
            if !subcategory_page.css(".categories-empty li a").empty?
              puts "Buscar si hay mas categorias"
            else
              puts "No hay subcategorias, escrapeamos blogs"
              subcategory_page.css("div#Listings.listings div.listing p a img").each do |blog|
                puts i+=1
                scrape_rss_blog blog['alt']
              end
              #y miramos si hay subcategorias en sidebar, hasta profundidad de nivel 1 (seguir mirando como hacer para profundidad de nivel 2)
              if !subcategory_page.css("div#ShowSpecificCategory1_SideBarTopLevelCategoriesPanel ul li a").empty?
                subcategory_page.css("div#ShowSpecificCategory1_SideBarTopLevelCategoriesPanel ul li a").each do |subcategory_sidebar|
                  subcategory_sidebar_page = Nokogiri::HTML(open("http://blogs.botw.org" + subcategory_sidebar['href']))
                  subcategory_page.css("div#Listings.listings div.listing p a img").each do |blog|
                    puts i+=1
                    scrape_rss_blog blog['alt']
                  end
                end
              end
              if !subcategory_page.css("div#ShowSpecificCategory1_SideBarMidLevelCategoriesPanel ul li a").empty?
                subcategory_page.css("div#ShowSpecificCategory1_SideBarMidLevelCategoriesPanel ul li a").each do |subcategory_sidebar|
                  subcategory_sidebar_page = Nokogiri::HTML(open("http://blogs.botw.org" + subcategory_sidebar['href']))
                  subcategory_page.css("div#Listings.listings div.listing p a img").each do |blog|
                    puts i+=1
                    scrape_rss_blog blog['alt']
                  end
                end
              end
              if !subcategory_page.css("div#ShowSpecificCategory1_SideBarBottomLevelCategoriesPanel ul li a").empty?
                subcategory_page.css("div#ShowSpecificCategory1_SideBarBottomLevelCategoriesPanel ul li a").each do |subcategory_sidebar|
                  subcategory_sidebar_page = Nokogiri::HTML(open("http://blogs.botw.org" + subcategory_sidebar['href']))
                  subcategory_page.css("div#Listings.listings div.listing p a img").each do |blog|
                    puts i+=1
                    scrape_rss_blog blog['alt']
                  end
                end
              end
            end
          end
        else
          puts "no tiene subcategorÃ­as, escrapeamos blogs"
          category_page.css("div#Listings.listings div.listing p a img").each do |blog|
            puts i+=1
            scrape_rss_blog blog['alt']
          end
          #y miramos si hay subcategorias en sidebar, hasta profundidad de nivel 1
          if !category_page.css("div#ShowSpecificCategory1_SideBarTopLevelCategoriesPanel ul li a").empty?
            category_page.css("div#ShowSpecificCategory1_SideBarTopLevelCategoriesPanel ul li a").each do |subcategory_sidebar|
              subcategory_page = Nokogiri::HTML(open("http://blogs.botw.org" + subcategory_sidebar['href']))
              subcategory_page.css("div#Listings.listings div.listing p a img").each do |blog|
                puts i+=1
                scrape_rss_blog blog['alt']
              end
            end
          end
        end
      end         
      
    rescue  Exception => e
      puts "Exception in scrape_botw"
      puts "Reintentamos dos veces, sino abortamos"
      puts e.backtrace.inspect
      puts e.message
      if reintentos > 0
        puts reintentos
        reintentos-=1
        sleep 2
        scrape_botw(1,reintentos)
      end
    end
  end #scrape_botw
  
  def scrape_rss_blog(rss_feed, *reintentos)
    begin    
      
      idiomas_disponibles = ["en"]
      
      if reintentos[0]==1
        reintentos = reintentos[1].to_i
      else
        reintentos = 2
      end
      
      post_summary = ""
      post_content = ""
      human_tags = Array.new

      scraped_blog = Blog.new
      
      # Reads the feed
      feed = Feedzirra::Feed.fetch_and_parse(rss_feed,{ :max_redirects => 3 })
      begin
        blog_name = feed.title
        blog_description = feed.description
      rescue Exception => e
        puts e.message
        puts e.backtrace.inspect
      end
      
      # If Blog doesn't exist, gets the name and description of the blog and saves it
      # If Blog exists, find it, and uses it for associate his posts 
      if Blog.find_by_name(blog_name)!=nil      
        scraped_blog = Blog.find_by_name(blog_name)       
      else
        scraped_blog = Blog.new
        scraped_blog.name = blog_name
        scraped_blog.description = blog_description
        scraped_blog.rss_feed = rss_feed
        scraped_blog.scraped_from = "http://" + URI.parse(rss_feed).host
        scraped_blog.save
      end         

      # Retrieves the posts, extracts the info, and saves them           
      feed.entries.each do |post|
      begin
        scraped_post = Post.new
        begin
          post_name = post.title.strip
          post_summary = strip_tags post.summary
          post_content = strip_tags post.content
          post_url = post.url
        rescue Exception => e
          puts e.message
          puts e.backtrace.inspect
        end
             
        return_values = extract_info_from_post(post_url, post_name)
        image_url = return_values[:image_url]
        info_to_wikify = return_values[:info_to_wikify]

        if image_url != nil          
          scraped_post.element_image = URI.parse(image_url)
        end
        
        I18n.locale = idiomas_disponibles[0]
        
        scraped_from = "http://" + URI.parse(post_url).host
        post_description = post_summary + post_content
        scraped_post.name = post_name
        scraped_post.description = post_description
        scraped_post.url = post_url
        scraped_post.link = post_url
        scraped_post.blog_id = scraped_blog.id
        scraped_post.info_to_wikify = scraped_post.name + info_to_wikify
        scraped_post.scraped_from = scraped_from
        scraped_post.persist(idiomas_disponibles,scraped_post.info_to_wikify,[])  
      rescue Exception => e
        puts "Failed Post"
        puts e.message
        puts e.backtrace.inspect
      end
    end 
    rescue  Exception => e
      puts e.backtrace.inspect
      puts e.message
      if reintentos > 0
        puts reintentos
        reintentos -= 1
        sleep 2
        scrape_rss_blog(1, reintentos)
      end
    end   
  end #scrape_rss_blog

  def extract_info_from_post(post_url, post_title)
    return_values = Hash.new
    # Builds post object
    post_html = open(post_url).read
    post_html.gsub!("\n", "")
    post_object = Nokogiri::HTML(post_html)
    
    # Cleans it
    post_object.at_css("body").traverse do |node|
      if node.text?
        node.content = node.content.gsub("\r", "")
        node.content = node.content.gsub("\n", "")
        node.content = node.content.gsub("\t", "")
        node.content = node.content.strip
      end
    end
    post_object.encoding = 'UTF-8'
    
    # Finds nodes that contain post_title
    post_title_matches = post_object.css("body")[0].search("[text()='" + post_title.strip + "']")

    # Gets the most suitable node
    best_match = post_title_matches[0]
    if post_title_matches[0].parent.name == "li"
      best_match = post_title_matches[1]
    end

    # Extracts a representative image and the content of the post
    if best_match != nil
      if !best_match.parent.css("img").empty?
        # Gets an array of images
        image_matches = best_match.parent.css("img")
        # Gets the first image with size higher than 2k
        image_url = first_big_image(post_url, image_matches)
        info_to_wikify = strip_tags best_match.parent.text.gsub(/\n/," ").gsub(/\s+/,' ').gsub("&"," ").gsub("%"," ")
      end
      if image_url == nil
        if !best_match.parent.parent.css("img").empty?
          # Gets an array of images
          image_matches = best_match.parent.parent.css("img")
        end
        # Gets the first image with size higher than 2k
        image_url = first_big_image(post_url, image_matches)
        info_to_wikify = strip_tags best_match.parent.parent.text.gsub(/\n/," ").gsub(/\s+/,' ').gsub("&"," ").gsub("%"," ")
      end          
    end
    
    return_values = {:image_url => image_url, :info_to_wikify => info_to_wikify}
  end

  # Gets the first image with size higher than 2k
  def first_big_image(post_url, image_matches)
    image_url = nil
    image_matches.each do |match|
      dummy_post = Post.new
      # Builds the URL
      url = match['src']
      if !match['src'].include? "http://"
        url = "http://" + URI.parse(post_url).host + match['src']
      end
      # Parses it
      dummy_post.element_image = URI.parse(url)
      # Assigns the return value if the image is bigger than 2k
      if dummy_post.element_image.size > 2000
        image_url = url
        break
      end
    end
    return image_url
  end

end #class ScraperBlogs